{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/soumithri/.local/share/virtualenvs/Thesis-cQCF3LGF/lib/python3.7/site-packages (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#!pip uninstall cdlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_values import (TV_SHOW, PRE_PROCESSED_FILE_NAME, LDA_FILE_NAME, \n",
    "                          OUT_DIR, GAMMA, TOLERANCE, ITERATIONS, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('YouNetflix_new', 'YouNetflix_new', 'YouNetflix_new', '../tvshows/output/')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TV_SHOW, PRE_PROCESSED_FILE_NAME, LDA_FILE_NAME, OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_DIR + PRE_PROCESSED_FILE_NAME + '_preprocessed_tweets_with_userid.csv', 'r') as infile:\n",
    "    df = pd.read_csv(infile, names=['userid', 'tweets'], usecols=['userid'], delimiter='|')\n",
    "    #df.tweets = df.tweets.apply(lambda x: literal_eval(x))\n",
    "    #df['tweets'] = df.tweets.str.replace(r'\\W+',' ')\n",
    "# Convert the tweet_doc into tweet_tokens and remove non_alphanumeric strings in the tokens\n",
    "#df['tweet_tokens'] = df['tweets'].apply(lambda x: x.split())\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 1048650\n",
      "Number of edges: 3369021\n",
      "Average in degree:   3.2127\n",
      "Average out degree:   3.2127\n",
      "\n",
      "Isolated nodes: 112586\n",
      "\n",
      "removing isolated nodes...\n",
      "\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 936064\n",
      "Number of edges: 3366312\n",
      "Average in degree:   3.5962\n",
      "Average out degree:   3.5962\n"
     ]
    }
   ],
   "source": [
    "def plot_graph(G):\n",
    "    pos = nx.spring_layout(G, k=0.3*1/np.sqrt(len(G.nodes())), iterations=20)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size = 50, with_labels=True)\n",
    "    #nx.draw_networkx_labels(G, pos)\n",
    "    nx.draw_networkx_edges(G, pos, with_labels=True, edge_color='black', arrows=True)\n",
    "    plt.rcParams['figure.figsize'] = [200, 200]\n",
    "    plt.title(\"Retweet Network drawn from 200 random nodes\", { 'fontsize': 20 })\n",
    "    plt.axis('off')\n",
    "    plt.rcParams[\"figure.figsize\"] = (30,30)\n",
    "    plt.show()\n",
    "    \n",
    "def remove_isolated_nodes(G):\n",
    "    print(nx.info(G))\n",
    "    isolated_nodes = list(nx.isolates(G))\n",
    "    print('\\nIsolated nodes: {}\\n'.format(len(isolated_nodes)))\n",
    "    print('removing isolated nodes...\\n')\n",
    "    G.remove_nodes_from(isolated_nodes)\n",
    "    G.remove_edges_from(G.selfloop_edges())\n",
    "    print(nx.info(G))\n",
    "    return G\n",
    "\n",
    "#%%\n",
    "graph = nx.read_graphml(OUT_DIR + TV_SHOW + '.graphml')\n",
    "\n",
    "graph = remove_isolated_nodes(graph)\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weights(graph):\n",
    "    degree_list = ['retweet_count', 'mention_count', 'reply_count', 'quote_count']\n",
    "    attrs = {}\n",
    "    for (node1,node2,*data) in graph.edges(data=True):\n",
    "        weight = sum([value for key, value in data[0].items() if key in degree_list])\n",
    "        attrs[(node1, node2)] = {'weight': weight}\n",
    "    nx.set_edge_attributes(graph, attrs)\n",
    "    return graph\n",
    "\n",
    "graph = add_weights(graph)\n",
    "#%%\n",
    "#%%\n",
    "# This functions takes the LDA topic model and returns document topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tvshows/output/YouNetflix_new.graphml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR + TV_SHOW + '.graphml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 936064\n",
      "Number of edges: 3366312\n",
      "Average in degree:   3.5962\n",
      "Average out degree:   3.5962\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_data(topic_model, corpus, dictionary, doc_topic_dists=None):\n",
    "\n",
    "    if not matutils.ismatrix(corpus):\n",
    "        corpus_csc = matutils.corpus2csc(corpus, num_terms=len(dictionary))\n",
    "    else:\n",
    "        corpus_csc = corpus\n",
    "        # Need corpus to be a streaming gensim list corpus for len and inference functions below:\n",
    "        corpus = matutils.Sparse2Corpus(corpus_csc)\n",
    "\n",
    "    beta = 0.01\n",
    "    fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)\n",
    "    term_freqs = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]\n",
    "    term_freqs[term_freqs == 0] = beta\n",
    "    doc_lengths = corpus_csc.sum(axis=0).A.ravel()\n",
    "\n",
    "    assert term_freqs.shape[0] == len(dictionary), 'Term frequencies and dictionary have different shape {} != {}'.format(\n",
    "        term_freqs.shape[0], len(dictionary))\n",
    "    assert doc_lengths.shape[0] == len(corpus), 'Document lengths and corpus have different sizes {} != {}'.format(\n",
    "        doc_lengths.shape[0], len(corpus))\n",
    "\n",
    "    if hasattr(topic_model, 'lda_alpha'):\n",
    "        num_topics = len(topic_model.lda_alpha)\n",
    "    else:\n",
    "        num_topics = topic_model.num_topics\n",
    "\n",
    "    if doc_topic_dists is None:\n",
    "        # If its an HDP model.\n",
    "        if hasattr(topic_model, 'lda_beta'):\n",
    "            gamma = topic_model.inference(corpus)\n",
    "        else:\n",
    "            gamma, _ = topic_model.inference(corpus)\n",
    "        doc_topic_dists = gamma / gamma.sum(axis=1)[:, None]\n",
    "    else:\n",
    "        if isinstance(doc_topic_dists, list):\n",
    "            doc_topic_dists = matutils.corpus2dense(doc_topic_dists, num_topics).T\n",
    "        elif issparse(doc_topic_dists):\n",
    "            doc_topic_dists = doc_topic_dists.T.todense()\n",
    "        doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)\n",
    "\n",
    "    assert doc_topic_dists.shape[1] == num_topics, 'Document topics and number of topics do not match {} != {}'.format(\n",
    "        doc_topic_dists.shape[1], num_topics)\n",
    "\n",
    "    # get the topic-term distribution straight from gensim without\n",
    "    # iterating over tuples\n",
    "    if hasattr(topic_model, 'lda_beta'):\n",
    "        topic = topic_model.lda_beta\n",
    "    else:\n",
    "        topic = topic_model.state.get_lambda()\n",
    "    topic = topic / topic.sum(axis=1)[:, None]\n",
    "    topic_term_dists = topic[:, fnames_argsort]\n",
    "\n",
    "    assert topic_term_dists.shape[0] == doc_topic_dists.shape[1]\n",
    "\n",
    "    return doc_topic_dists\n",
    "\n",
    "def get_doc_topic_dist(OUT_DIR=OUT_DIR):\n",
    "    lda_dict = corpora.Dictionary.load(OUT_DIR + LDA_FILE_NAME + '.dict') \n",
    "    lda_corpus = corpora.MmCorpus(OUT_DIR + LDA_FILE_NAME + '.mm')\n",
    "    lda = LdaMulticore.load(OUT_DIR + LDA_FILE_NAME + '.lda')\n",
    "    return _extract_data(topic_model=lda, dictionary=lda_dict, corpus=lda_corpus)\n",
    "#%%\n",
    "def get_DT_row_norm(doc_topic_dist):\n",
    "    DT_row_norm = np.asmatrix(normalize(doc_topic_dist, axis=1, norm='l1'))\n",
    "    return DT_row_norm\n",
    "\n",
    "def get_DT_col_norm(doc_topic_dist):\n",
    "    DT_col_norm = np.asmatrix(normalize(doc_topic_dist, axis=0, norm='l1'))\n",
    "    return DT_col_norm\n",
    "\n",
    "def get_sim(DT_row_norm, i, j, k):\n",
    "    sim = 1 - abs(DT_row_norm.item((i,k))-DT_row_norm.item(j,k))\n",
    "    return sim    \n",
    "\n",
    "def get_weight(nodei, nodej, graph):\n",
    "    ''' Adds weights to the Transition matrix by accepting two nodes: node1, nodej.\n",
    "    weight is computed as follows:\n",
    "    \n",
    "        weight = (sum of weighted in-degrees of nodej)/(sum of weighted degrees of node1)\n",
    "        Returns 0.0 if both numerator and denominator of the above expression is 0\n",
    "    '''\n",
    "    if nx.has_path(graph, nodei, nodej) and graph.has_edge(nodei, nodej):\n",
    "        #print(nodei , nodej)\n",
    "        return (graph.get_edge_data(nodei, nodej)['weight'] / graph.out_degree(nodei, weight='weight'))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_Pt(DT_row_norm, k, graph, data=df):\n",
    "    size = DT_row_norm.shape[0]\n",
    "    trans_mat = np.zeros((size, size))\n",
    "    for i in range(0, size):\n",
    "        for j in range(0, size):\n",
    "            if graph.has_node(str(data['userid'].iloc[i])) and graph.has_node(str(data['userid'].iloc[j])):\n",
    "                trans_mat[i][j] = get_weight(str(data['userid'].iloc[i]), str(data['userid'].iloc[j]), graph) * get_sim(DT_row_norm, i, j, k)   \n",
    "            else:\n",
    "                trans_mat[i][j] = 0.0      \n",
    "    return trans_mat\n",
    "\n",
    "\n",
    "def get_TRt(gamma, trans_mat, Et, iter=1000, tolerance=1e-16):\n",
    "    old_TRt = Et\n",
    "    i = 0\n",
    "    while i < iter:\n",
    "        TRt = (gamma*np.dot(trans_mat,old_TRt)) + ((1 - gamma) * Et)\n",
    "        euclidean_dis = np.linalg.norm(TRt - old_TRt)\n",
    "        if euclidean_dis < tolerance: \n",
    "            print('Topic Rank vectors have converged...')\n",
    "            break\n",
    "        old_TRt = TRt\n",
    "        i += 1\n",
    "    return TRt\n",
    "\n",
    "def get_TR(DT_row_norm, DT_col_norm, num_topics, gamma, tolerance, graph, data=df):\n",
    "    for k in range(0, num_topics):\n",
    "        trans_mat = get_Pt(DT_row_norm, k, graph, data)\n",
    "        Et = DT_col_norm[:,k]\n",
    "        if k==0: TR = get_TRt(gamma, trans_mat, Et)\n",
    "        else: TR = np.concatenate((TR, get_TRt(gamma, trans_mat, Et)), axis=1)\n",
    "    return TR\n",
    "\n",
    "def get_TR_sum(TR, samples, num_topics):\n",
    "    TR_sum = [0 for i in range(0, samples)]\n",
    "    for i in range(0, num_topics):\n",
    "        for j in range(0, samples):\n",
    "            TR_sum[j] += TR[i][j]\n",
    "    TR_sum.sort()\n",
    "    return TR_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "doc_topic_dist = get_doc_topic_dist(OUT_DIR)\n",
    "DT_row_norm = get_DT_row_norm(doc_topic_dist)\n",
    "DT_col_norm = get_DT_col_norm(doc_topic_dist)\n",
    "#%%\n",
    "# Check the transition matrix\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3290, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n",
      "Topic Rank vectors have converged...\n"
     ]
    }
   ],
   "source": [
    "TR = get_TR(DT_row_norm, DT_col_norm, graph=graph, data=df, \n",
    "            num_topics=NUM_TOPICS, gamma=GAMMA, tolerance=TOLERANCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_sum = np.sum(TR, axis=1).tolist()\n",
    "TR_sum = [item for sublist in TR_sum for item in sublist]\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_random_walk_for_each_topic(transition_matrix, state_topic_vector,\n",
    "                                       damping_factor=0.85, iterations=100, tolerance=1e-6):\n",
    "  while iterations > 0:\n",
    "     new_state_topic_vector = damping_factor * np.dot(transition_matrix, state_topic_vector) + \\\n",
    "                                                         (1 - damping_factor) * state_topic_vector\n",
    "     distance = np.linalg.norm(new_state_topic_vector - state_topic_vector)\n",
    "     if distance < tolerance:\n",
    "         break\n",
    "     else:\n",
    "         state_topic_vector = new_state_topic_vector\n",
    "\n",
    "  return new_state_topic_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936064"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_nodes = pd.DataFrame(data=np.zeros((graph.number_of_nodes(), NUM_TOPICS)), index=graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_nodes.to_csv(TV_SHOW + '_graph_nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=DT_row_norm, index=df.userid.tolist()).to_csv(TV_SHOW + '_topic_frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_frame = pd.DataFrame(data=TR, index=df.userid.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_frame.to_csv(TV_SHOW + '_topic_rank_frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df_graph_nodes, df_topic_frame], axis=1).iloc[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.fillna(np.float64(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = nx.google_matrix(graph, nodelist=final_df.index.tolist(), weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TV_SHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_nodes.to_csv(TV_SHOW + '_graph_nodes_with_no_isolated_nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(graph.degree(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "( list(df_topic_frame.index) in list(graph.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_frame = pd.DataFrame(data=DT_row_norm, index=df.userid.tolist())\n",
    "df_topic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [str(x) for x in df_topic_frame.index]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(graph.nodes()) - set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in graph.edges(['18969235'], data=True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
