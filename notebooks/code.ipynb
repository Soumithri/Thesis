{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from scipy.spatial.distance import cdist\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_values import (\n",
    "    TV_SHOW, \n",
    "    PRE_PROCESSED_FILE_NAME, \n",
    "    LDA_FILE_NAME, \n",
    "    OUT_DIR, \n",
    "    GAMMA, \n",
    "    TOLERANCE, \n",
    "    ITERATIONS, \n",
    "    NUM_TOPICS,\n",
    "    GRAPHML_FILE,\n",
    "    GRAPH_NODE_FILE,\n",
    "    TOPIC_VEC_FILE,\n",
    "    TWITTER_RANK_FILE,\n",
    "    NUM_OF_INFLUENTIAL_NODES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_INFLUENTIAL_NODES = 100\n",
    "threshold_percentile_for_merge = 50\n",
    "threshold_percentile_for_split = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolated_nodes(G):\n",
    "    print(nx.info(G))\n",
    "    isolated_nodes = list(nx.isolates(G))\n",
    "    print('\\nIsolated nodes: {}\\n'.format(len(isolated_nodes)))\n",
    "    print('removing isolated nodes...\\n')\n",
    "    G.remove_nodes_from(isolated_nodes)\n",
    "    print(nx.info(G))\n",
    "    return G\n",
    "\n",
    "def load_graph(graph_file = GRAPHML_FILE):\n",
    "\n",
    "    graph = nx.read_graphml(GRAPHML_FILE)\n",
    "    print(nx.info(graph))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataframes :-\n",
    "## user_id_df, graph_nodes_df, twitter_rank_df, topic_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_frame(file, graph_node_file=GRAPH_NODE_FILE):\n",
    "    graph = pd.read_csv(graph_node_file)\n",
    "    graph = graph.rename(columns = {'Unnamed: 0':'userid'})\n",
    "    topic = pd.read_csv(file)\n",
    "    topic = topic.rename(columns = {'Unnamed: 0':'userid'})\n",
    "    columns = topic.columns\n",
    "    new = pd.merge(graph,topic,on = 'userid',how = 'left')\n",
    "    dic = {'0_x':'0_y','1_x':'1_y','2_x':'2_y','3_x':'3_y','4_x':'4_y','5_x':'5_y',\n",
    "           '6_x':'6_y','6_x':'6_y','7_x':'7_y','8_x':'8_y','9_x':'9_y'}\n",
    "    for i in dic:\n",
    "        new[i] = new[i] + new[dic[i]]\n",
    "    new = new.drop(columns = dic.values())\n",
    "    new.fillna(0.11111111)\n",
    "    new.set_index('userid', inplace=True)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_influential_nodes(graph, df, twitter_rank_df, num_topics=NUM_TOPICS, \n",
    "                          num_of_influential_nodes=NUM_OF_INFLUENTIAL_NODES):\n",
    "    topic_rank = twitter_rank_df.values\n",
    "    topic_rank_sum = np.sum(topic_rank/num_topics, axis=1)\n",
    "    average_twitter_rank = np.array(topic_rank_sum[:])\n",
    "\n",
    "    df['avg_twitter_rank'] = average_twitter_rank\n",
    "    influential_nodes_index = np.argsort(average_twitter_rank, axis=0).reshape(\n",
    "        len(average_twitter_rank),1)[:NUM_OF_INFLUENTIAL_NODES, :]\n",
    "\n",
    "    influential_nodes_index = [int(i) for i in influential_nodes_index]\n",
    "    influential_nodes = df.iloc[influential_nodes_index].index.tolist()\n",
    "    \n",
    "    return influential_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Community detection algorithm here\n",
    "\n",
    "def step1_assign_initial_communities(graph, df, influential_nodes):\n",
    "    # Initial community assignment\n",
    "    influential_nodes = [int(i) for i in influential_nodes]\n",
    "    communities = {k: [k] for k in influential_nodes}\n",
    "\n",
    "    # Remaining nodes\n",
    "    remaining_nodes = set(df.index) - set(influential_nodes)\n",
    "\n",
    "    # Get the weighted adjacency matrix\n",
    "    adjacency_matrix = nx.adjacency_matrix(graph, weight='weight').todense()\n",
    "    adjacency_matrix_df = pd.DataFrame(data=adjacency_matrix, index=df.index, \n",
    "                                       columns=df.index)\n",
    "\n",
    "    # Get nodes weight with influential users\n",
    "    nodes_weight_with_influential_nodes_df = adjacency_matrix_df[influential_nodes]\n",
    "    nodes_weight_with_influential_nodes_df['max_weight_with_influencer'] = nodes_weight_with_influential_nodes_df[\n",
    "        nodes_weight_with_influential_nodes_df > 0].idxmax(axis=1)\n",
    "    nodes_weight_with_influential_nodes_df['max_weight_with_influencer'].fillna(False, inplace=True)\n",
    "    # Main algorithm here\n",
    "\n",
    "\n",
    "    for node in remaining_nodes:\n",
    "      # add this node to an influencerâ€™s community if this influencer \n",
    "      # and this node have the highest edge weight\n",
    "      influencer_with_max_weight_with_node = nodes_weight_with_influential_nodes_df[\n",
    "          'max_weight_with_influencer'].loc[node]\n",
    "      if isinstance(influencer_with_max_weight_with_node, np.int64):\n",
    "        communities[influencer_with_max_weight_with_node].append(node)\n",
    "    \n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2_split_community(df, communities, threshold_percentile_for_split):\n",
    "\n",
    "    # Split these initial communities based on topic vectors.  \n",
    "    # Given a community of m nodes, we can compute the pairwise cosine-distance of \n",
    "    # the topical vectors. This will give us m(m-2)/2 distances.  \n",
    "    # We then remove a node if its cosine distances from all its neighbors \n",
    "    # are below a threshold, say, the first quartile of all the m(m-2)/2 distances.\n",
    "\n",
    "\n",
    "    topic_vectors = df.iloc[:,:-1].values\n",
    "    cosine_sim = np.array([]).reshape(len(df),0)\n",
    "    for node in df.index.tolist():\n",
    "      cosine_sim = np.c_[cosine_sim, 1 - cdist(topic_vectors, np.matrix(df.loc[node])[:,:NUM_TOPICS])]\n",
    "\n",
    "    cosine_sim_df = pd.DataFrame(data=cosine_sim, index=df.index, columns=df.index)\n",
    "    mapped_nodes_in_communities = list(itertools.chain(*communities.values()))\n",
    "    community_cosine_sim_df = cosine_sim_df.loc[mapped_nodes_in_communities][mapped_nodes_in_communities]\n",
    "    \n",
    "    \n",
    "    split_threshold = np.percentile(community_cosine_sim_df.values, threshold_percentile_for_split)\n",
    "    #splitting here\n",
    "    updated_communities = deepcopy(communities)\n",
    "    for seed_node, community in communities.items():\n",
    "      if len(community) == 1:\n",
    "        print('Cannot split for community since it has community: {} since it has only one node'.format(community))\n",
    "      else:\n",
    "        # split the community based on topic vectors within a community\n",
    "        for count_nodes, community_node in enumerate(community):\n",
    "          if community_node == seed_node:\n",
    "            continue\n",
    "          is_cos_dist_bigger_than_threshold = list(community_cosine_sim_df.loc[community_node] > split_threshold)\n",
    "          if False in is_cos_dist_bigger_than_threshold:\n",
    "            print('Splitting node: {} from community: {}'.format(community_node, updated_communities[seed_node]))\n",
    "            updated_communities[community_node] = [community_node]\n",
    "            updated_communities[seed_node].remove(community_node)\n",
    "    \n",
    "    return updated_communities, community_cosine_sim_df, mapped_nodes_in_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_cosine_distance(comm1, community_cosine_sim_df, merge_threshold, mapped_nodes_in_communities):\n",
    "  if not isinstance(comm1, set):\n",
    "    comm1 = set(comm1)\n",
    "  remaining_list = list(set(mapped_nodes_in_communities)-comm1)\n",
    "  #print('*****',comm1)\n",
    "  min_cosine_dist_from_comm1 = community_cosine_sim_df[community_cosine_sim_df.index.isin(list(comm1))][remaining_list].idxmax(axis=1).values\n",
    "  #print('*****',min_cosine_dist_from_comm1)\n",
    "  min_distance_list = []\n",
    "  for i in zip(comm1, min_cosine_dist_from_comm1):\n",
    "    min_distance_list.append(community_cosine_sim_df[i[0]][i[1]])\n",
    "  #print('*****',min_distance_list)\n",
    "  community_to_merge = None\n",
    "  if min(min_distance_list) > merge_threshold:\n",
    "    community_to_merge = min_cosine_dist_from_comm1[min_distance_list.index(min(min_distance_list))]\n",
    "  return community_to_merge\n",
    "\n",
    "def step3_merge_communities(updated_communities, community_cosine_sim_df, threshold_percentile_for_merge, \n",
    "                            mapped_nodes_in_communities):\n",
    "    \n",
    "    merge_threshold = np.percentile(community_cosine_sim_df.values, threshold_percentile_for_split)\n",
    "    test_communities = deepcopy(updated_communities)\n",
    "    #print(test_communities)\n",
    "    for seed_node, community in updated_communities.items():\n",
    "      if seed_node in test_communities:\n",
    "        community_to_merge_with_seed = get_min_cosine_distance(community, community_cosine_sim_df, \n",
    "                                                               merge_threshold, mapped_nodes_in_communities)\n",
    "        #print(seed_node, community_to_merge_with_seed)\n",
    "        if community_to_merge_with_seed:\n",
    "          seed_node_to_merge = [key for key, value in test_communities.items() if community_to_merge_with_seed in value][0]\n",
    "          #print(seed_node, test_communities)\n",
    "          merging_communities = test_communities.pop(seed_node_to_merge, None)\n",
    "          #print(seed_node, merging_communities)\n",
    "          if merging_communities:\n",
    "            test_communities[seed_node].extend(merging_communities)\n",
    "    \n",
    "    return test_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partitions(df, communities):\n",
    "    partitions = dict()\n",
    "\n",
    "    for k, v in communities.items():\n",
    "      for i in v:\n",
    "        partitions[int(i)] = int(k) \n",
    "        \n",
    "    for i in set(df.index)-set(partitions.keys()):\n",
    "        partitions[i] = i\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conductance(graph, partitions):\n",
    "  conductances_list = []\n",
    "  conductances_keys = []\n",
    "  for key, coms in partitions.items():\n",
    "    try:\n",
    "        conductances_list.append(nx.conductance(graph, coms))\n",
    "    except ZeroDivisionError:\n",
    "        conductances_list.append(np.NAN)\n",
    "        conductances_keys.append(key)\n",
    "    else:\n",
    "        conductances_keys.append(key)\n",
    "  conductance_measures = dict(zip(conductances_keys, conductances_list))\n",
    "  if not conductance_measures:\n",
    "        return {}\n",
    "  return {'all_conductances': conductance_measures, 'min_conductance': min(conductance_measures.values()),\n",
    "          'max_conductance': max(conductance_measures.values()), \n",
    "          'avg_conductance': sum(conductance_measures.values())/len(partitions)}\n",
    "\n",
    "def get_triangle_participation_ratio(graph, partitions):\n",
    "  if nx.is_directed(graph):\n",
    "    graph = nx.to_undirected(graph)\n",
    "  tpr_measures = dict(zip(partitions.keys(), [triangle_participation_ratio(graph, coms) \n",
    "                                              for coms in partitions.values()]))\n",
    "  return {'all_tprs': tpr_measures, 'min_tpr': min(tpr_measures.values()),\n",
    "          'max_tpr': max(tpr_measures.values()), 'avg_tpr': sum(tpr_measures.values())/len(partitions)}\n",
    "\n",
    "\n",
    "def triangle_participation_ratio(graph, coms):\n",
    "  cls = nx.triangles(graph, coms)\n",
    "  nc = [n for n in cls if cls[n] > 0]\n",
    "  return float(len(nc))/len(coms)\n",
    "\n",
    "\n",
    "def get_community_modularity(graph, partitions):\n",
    "  modularities = {}\n",
    "  try:\n",
    "      modularities = {'modularity': nx.algorithms.community.modularity(graph, [set(com) \n",
    "                                                                               for com in partitions.values()])}\n",
    "  except:\n",
    "    return {}\n",
    "  return modularities\n",
    "\n",
    "def get_surprise(graph, partitions):\n",
    "  m = graph.number_of_edges()\n",
    "  n = graph.number_of_nodes()\n",
    "\n",
    "  q = 0\n",
    "  qa = 0\n",
    "  sp = 0\n",
    "\n",
    "  for community in partitions.values():\n",
    "      c = nx.subgraph(graph, community)\n",
    "      mc = c.number_of_edges()\n",
    "      nc = c.number_of_nodes()\n",
    "\n",
    "      q += mc\n",
    "      qa += scipy.special.comb(nc, 2, exact=True)\n",
    "  try:\n",
    "      q = q / m\n",
    "      qa = qa / scipy.special.comb(n, 2, exact=True)\n",
    "\n",
    "      sp = m * (q * np.log(q / qa) + (1 - q) * np.log2((1 - q) / (1 - qa)))\n",
    "  except ZeroDivisionError:\n",
    "      return {'asymptotic_surprise': np.NAN}\n",
    "  return {'asymptotic_surprise': sp}\n",
    "\n",
    "def get_significance(graph, partitions):\n",
    "\n",
    "  m = graph.number_of_edges()\n",
    "\n",
    "  binom = scipy.special.comb(m, 2, exact=True)\n",
    "  p = m / binom\n",
    "\n",
    "  q = 0\n",
    "\n",
    "  for community in partitions.values():\n",
    "      try:\n",
    "          c = nx.subgraph(graph, community)\n",
    "          nc = c.number_of_nodes()\n",
    "          mc = c.number_of_edges()\n",
    "\n",
    "          binom_c = scipy.special.comb(nc, 2, exact=True)\n",
    "          pc = mc / binom_c\n",
    "\n",
    "          q += binom_c * (pc * np.log(pc / p) + (1 - pc) * np.log((1 - pc) / (1 - p)))\n",
    "      except ZeroDivisionError:\n",
    "          return {'significance': np.NAN}\n",
    "  return {'significance': q}\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(graph, partitions):\n",
    "  return dict(chain(get_conductance(graph, partitions).items(), \n",
    "                    get_triangle_participation_ratio(graph, partitions).items(), \n",
    "                    get_community_modularity(graph, partitions).items(), \n",
    "                    get_surprise(graph, partitions).items(),\n",
    "                    get_significance(graph, partitions).items())\n",
    "             )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stranger Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 1969\n",
      "Number of edges: 457\n",
      "Average in degree:   0.2321\n",
      "Average out degree:   0.2321\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 1969\n",
      "Number of edges: 457\n",
      "Average in degree:   0.2321\n",
      "Average out degree:   0.2321\n",
      "\n",
      "Isolated nodes: 1592\n",
      "\n",
      "removing isolated nodes...\n",
      "\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 377\n",
      "Number of edges: 457\n",
      "Average in degree:   1.2122\n",
      "Average out degree:   1.2122\n"
     ]
    }
   ],
   "source": [
    "st_graph = load_graph(GRAPHML_FILE)\n",
    "st_graph = remove_isolated_nodes(st_graph)\n",
    "st_topic_vec_df = get_topic_frame(TOPIC_VEC_FILE, GRAPH_NODE_FILE)\n",
    "st_twitter_rank_df = get_topic_frame(TWITTER_RANK_FILE, GRAPH_NODE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "influential_nodes = get_influential_nodes(st_graph, st_topic_vec_df, st_twitter_rank_df, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumithri/.local/share/virtualenvs/Thesis-cQCF3LGF/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/soumithri/.local/share/virtualenvs/Thesis-cQCF3LGF/lib/python3.7/site-packages/pandas/core/generic.py:6130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "initial_communities = step1_assign_initial_communities(st_graph, st_topic_vec_df, influential_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot split for community since it has community: [775072732916572160] since it has only one node\n",
      "Cannot split for community since it has community: [3974634941] since it has only one node\n",
      "Cannot split for community since it has community: [928458410013675521] since it has only one node\n",
      "Cannot split for community since it has community: [848606923087908864] since it has only one node\n",
      "Cannot split for community since it has community: [3228848510] since it has only one node\n",
      "Cannot split for community since it has community: [2747410528] since it has only one node\n",
      "Cannot split for community since it has community: [767430013616394240] since it has only one node\n",
      "Cannot split for community since it has community: [856578556427632643] since it has only one node\n",
      "Cannot split for community since it has community: [2298700003] since it has only one node\n",
      "Cannot split for community since it has community: [134787107] since it has only one node\n",
      "Cannot split for community since it has community: [4693956132] since it has only one node\n",
      "Cannot split for community since it has community: [4090167852] since it has only one node\n",
      "Cannot split for community since it has community: [2338760659] since it has only one node\n",
      "Cannot split for community since it has community: [345056881] since it has only one node\n",
      "Cannot split for community since it has community: [2251175080] since it has only one node\n",
      "Cannot split for community since it has community: [327467138] since it has only one node\n",
      "Cannot split for community since it has community: [2287341380] since it has only one node\n",
      "Cannot split for community since it has community: [2982624985] since it has only one node\n",
      "Cannot split for community since it has community: [2381452592] since it has only one node\n",
      "Cannot split for community since it has community: [2852480889] since it has only one node\n",
      "Cannot split for community since it has community: [3119777010] since it has only one node\n",
      "Cannot split for community since it has community: [197251803] since it has only one node\n",
      "Cannot split for community since it has community: [2301233508] since it has only one node\n",
      "Cannot split for community since it has community: [769742644243300356] since it has only one node\n",
      "Cannot split for community since it has community: [1654231795] since it has only one node\n",
      "Cannot split for community since it has community: [93761096] since it has only one node\n",
      "Cannot split for community since it has community: [3229506709] since it has only one node\n",
      "Cannot split for community since it has community: [336872587] since it has only one node\n",
      "Cannot split for community since it has community: [3382144539] since it has only one node\n",
      "Cannot split for community since it has community: [3607069393] since it has only one node\n",
      "Cannot split for community since it has community: [1268768983] since it has only one node\n",
      "Cannot split for community since it has community: [501142275] since it has only one node\n",
      "Cannot split for community since it has community: [2382020768] since it has only one node\n",
      "Cannot split for community since it has community: [885193729098752002] since it has only one node\n",
      "Cannot split for community since it has community: [2980610033] since it has only one node\n",
      "Cannot split for community since it has community: [751721364] since it has only one node\n",
      "Cannot split for community since it has community: [742920518353879041] since it has only one node\n",
      "Cannot split for community since it has community: [2374706366] since it has only one node\n",
      "Cannot split for community since it has community: [3033531971] since it has only one node\n",
      "Cannot split for community since it has community: [228891747] since it has only one node\n",
      "Cannot split for community since it has community: [375710976] since it has only one node\n",
      "Cannot split for community since it has community: [3007579465] since it has only one node\n",
      "Cannot split for community since it has community: [707632407] since it has only one node\n",
      "Cannot split for community since it has community: [895406294349344768] since it has only one node\n",
      "Cannot split for community since it has community: [3033268071] since it has only one node\n",
      "Cannot split for community since it has community: [762213374] since it has only one node\n",
      "Cannot split for community since it has community: [885036683250606080] since it has only one node\n",
      "Cannot split for community since it has community: [418202983] since it has only one node\n",
      "Cannot split for community since it has community: [576840165] since it has only one node\n",
      "Cannot split for community since it has community: [1317359371] since it has only one node\n",
      "Cannot split for community since it has community: [2257011015] since it has only one node\n",
      "Cannot split for community since it has community: [813970730] since it has only one node\n",
      "Cannot split for community since it has community: [726540672] since it has only one node\n",
      "Cannot split for community since it has community: [2268164780] since it has only one node\n",
      "Cannot split for community since it has community: [807298695771385856] since it has only one node\n",
      "Cannot split for community since it has community: [2837077179] since it has only one node\n",
      "Cannot split for community since it has community: [791138905437462530] since it has only one node\n",
      "Cannot split for community since it has community: [763875106900672512] since it has only one node\n",
      "Cannot split for community since it has community: [3302228866] since it has only one node\n",
      "Cannot split for community since it has community: [3177240188] since it has only one node\n",
      "Cannot split for community since it has community: [2964100343] since it has only one node\n",
      "Cannot split for community since it has community: [711387485700366337] since it has only one node\n",
      "Cannot split for community since it has community: [780136472615804928] since it has only one node\n",
      "Cannot split for community since it has community: [18025029] since it has only one node\n",
      "Cannot split for community since it has community: [757160909139615744] since it has only one node\n",
      "Cannot split for community since it has community: [361591259] since it has only one node\n",
      "Cannot split for community since it has community: [2345722098] since it has only one node\n",
      "Cannot split for community since it has community: [1634283092] since it has only one node\n",
      "Cannot split for community since it has community: [832933001080168450] since it has only one node\n",
      "Cannot split for community since it has community: [878708341756878849] since it has only one node\n",
      "Cannot split for community since it has community: [17629149] since it has only one node\n",
      "Cannot split for community since it has community: [726945992352870400] since it has only one node\n",
      "Cannot split for community since it has community: [955382688] since it has only one node\n",
      "Cannot split for community since it has community: [556412825] since it has only one node\n",
      "Cannot split for community since it has community: [1138293733] since it has only one node\n",
      "Cannot split for community since it has community: [851999658708783104] since it has only one node\n",
      "Cannot split for community since it has community: [440526007] since it has only one node\n",
      "Cannot split for community since it has community: [525498287] since it has only one node\n",
      "Cannot split for community since it has community: [324068936] since it has only one node\n",
      "Cannot split for community since it has community: [3294535848] since it has only one node\n",
      "Cannot split for community since it has community: [2439521113] since it has only one node\n",
      "Cannot split for community since it has community: [610473208] since it has only one node\n",
      "Cannot split for community since it has community: [593332602] since it has only one node\n",
      "Cannot split for community since it has community: [2459036575] since it has only one node\n",
      "Cannot split for community since it has community: [3401646184] since it has only one node\n",
      "Cannot split for community since it has community: [450454763] since it has only one node\n",
      "Cannot split for community since it has community: [896101980808130560] since it has only one node\n",
      "Cannot split for community since it has community: [374318146] since it has only one node\n",
      "Cannot split for community since it has community: [3016439087] since it has only one node\n",
      "Cannot split for community since it has community: [288884420] since it has only one node\n",
      "Cannot split for community since it has community: [713989239180894212] since it has only one node\n",
      "Cannot split for community since it has community: [149406681] since it has only one node\n",
      "Cannot split for community since it has community: [3939384136] since it has only one node\n",
      "Cannot split for community since it has community: [1704039948] since it has only one node\n",
      "Cannot split for community since it has community: [910283586] since it has only one node\n",
      "Cannot split for community since it has community: [511238014] since it has only one node\n",
      "Cannot split for community since it has community: [702936312786120704] since it has only one node\n",
      "Cannot split for community since it has community: [578146511] since it has only one node\n",
      "Cannot split for community since it has community: [804182695173558272] since it has only one node\n",
      "Cannot split for community since it has community: [984062040] since it has only one node\n"
     ]
    }
   ],
   "source": [
    "communities_after_split, community_cosine_sim_df, mapped_nodes = step2_split_community(\n",
    "    st_topic_vec_df, initial_communities,threshold_percentile_for_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "communties_after_merge = step3_merge_communities(communities_after_split, community_cosine_sim_df,\n",
    "                                                 threshold_percentile_for_merge, mapped_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{775072732916572160: [775072732916572160, 767430013616394240],\n",
       " 3974634941: [3974634941, 885193729098752002],\n",
       " 928458410013675521: [928458410013675521, 501142275],\n",
       " 3228848510: [3228848510, 848606923087908864, 2374706366],\n",
       " 856578556427632643: [856578556427632643, 2338760659],\n",
       " 134787107: [134787107, 4693956132],\n",
       " 4090167852: [4090167852, 1654231795],\n",
       " 327467138: [327467138, 2298700003, 2251175080],\n",
       " 2982624985: [2982624985, 2381452592],\n",
       " 2301233508: [2301233508, 3119777010, 197251803],\n",
       " 93761096: [93761096, 3229506709],\n",
       " 336872587: [336872587, 345056881, 576840165],\n",
       " 3607069393: [3607069393, 2980610033],\n",
       " 2382020768: [2382020768, 742920518353879041],\n",
       " 751721364: [751721364, 804182695173558272],\n",
       " 895406294349344768: [895406294349344768, 3007579465, 3033531971, 228891747],\n",
       " 3033268071: [3033268071, 3382144539, 1268768983],\n",
       " 2257011015: [2257011015, 813970730],\n",
       " 2268164780: [2268164780, 726540672, 762213374, 18025029],\n",
       " 2837077179: [2837077179, 3294535848],\n",
       " 791138905437462530: [791138905437462530,\n",
       "  375710976,\n",
       "  769742644243300356,\n",
       "  2287341380,\n",
       "  2852480889],\n",
       " 763875106900672512: [763875106900672512, 3177240188],\n",
       " 3302228866: [3302228866, 878708341756878849],\n",
       " 711387485700366337: [711387485700366337, 418202983, 713989239180894212],\n",
       " 780136472615804928: [780136472615804928, 361591259],\n",
       " 2345722098: [2345722098, 1634283092],\n",
       " 832933001080168450: [832933001080168450,\n",
       "  757160909139615744,\n",
       "  2747410528,\n",
       "  2964100343],\n",
       " 17629149: [17629149, 610473208],\n",
       " 851999658708783104: [851999658708783104,\n",
       "  807298695771385856,\n",
       "  1317359371,\n",
       "  707632407,\n",
       "  885036683250606080],\n",
       " 525498287: [525498287, 726945992352870400, 955382688],\n",
       " 324068936: [324068936, 1138293733, 3401646184],\n",
       " 593332602: [593332602, 896101980808130560],\n",
       " 450454763: [450454763, 2439521113, 556412825, 440526007],\n",
       " 3016439087: [3016439087, 374318146, 288884420],\n",
       " 149406681: [149406681, 578146511],\n",
       " 3939384136: [3939384136, 1704039948],\n",
       " 910283586: [910283586, 2459036575, 702936312786120704],\n",
       " 511238014: [511238014, 984062040]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communties_after_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = make_partitions(st_topic_vec_df, communties_after_merge)\n",
    "coms_dict = deepcopy(communties_after_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_conductances': {775072732916572160: nan,\n",
       "  3974634941: nan,\n",
       "  928458410013675521: nan,\n",
       "  3228848510: nan,\n",
       "  856578556427632643: nan,\n",
       "  134787107: nan,\n",
       "  4090167852: nan,\n",
       "  327467138: nan,\n",
       "  2982624985: nan,\n",
       "  2301233508: nan,\n",
       "  93761096: nan,\n",
       "  336872587: nan,\n",
       "  3607069393: nan,\n",
       "  2382020768: nan,\n",
       "  751721364: nan,\n",
       "  895406294349344768: nan,\n",
       "  3033268071: nan,\n",
       "  2257011015: nan,\n",
       "  2268164780: nan,\n",
       "  2837077179: nan,\n",
       "  791138905437462530: nan,\n",
       "  763875106900672512: nan,\n",
       "  3302228866: nan,\n",
       "  711387485700366337: nan,\n",
       "  780136472615804928: nan,\n",
       "  2345722098: nan,\n",
       "  832933001080168450: nan,\n",
       "  17629149: nan,\n",
       "  851999658708783104: nan,\n",
       "  525498287: nan,\n",
       "  324068936: nan,\n",
       "  593332602: nan,\n",
       "  450454763: nan,\n",
       "  3016439087: nan,\n",
       "  149406681: nan,\n",
       "  3939384136: nan,\n",
       "  910283586: nan,\n",
       "  511238014: nan},\n",
       " 'min_conductance': nan,\n",
       " 'max_conductance': nan,\n",
       " 'avg_conductance': nan,\n",
       " 'all_tprs': {775072732916572160: 0.0,\n",
       "  3974634941: 0.0,\n",
       "  928458410013675521: 0.0,\n",
       "  3228848510: 0.0,\n",
       "  856578556427632643: 0.0,\n",
       "  134787107: 0.0,\n",
       "  4090167852: 0.0,\n",
       "  327467138: 0.0,\n",
       "  2982624985: 0.0,\n",
       "  2301233508: 0.0,\n",
       "  93761096: 0.0,\n",
       "  336872587: 0.0,\n",
       "  3607069393: 0.0,\n",
       "  2382020768: 0.0,\n",
       "  751721364: 0.0,\n",
       "  895406294349344768: 0.0,\n",
       "  3033268071: 0.0,\n",
       "  2257011015: 0.0,\n",
       "  2268164780: 0.0,\n",
       "  2837077179: 0.0,\n",
       "  791138905437462530: 0.0,\n",
       "  763875106900672512: 0.0,\n",
       "  3302228866: 0.0,\n",
       "  711387485700366337: 0.0,\n",
       "  780136472615804928: 0.0,\n",
       "  2345722098: 0.0,\n",
       "  832933001080168450: 0.0,\n",
       "  17629149: 0.0,\n",
       "  851999658708783104: 0.0,\n",
       "  525498287: 0.0,\n",
       "  324068936: 0.0,\n",
       "  593332602: 0.0,\n",
       "  450454763: 0.0,\n",
       "  3016439087: 0.0,\n",
       "  149406681: 0.0,\n",
       "  3939384136: 0.0,\n",
       "  910283586: 0.0,\n",
       "  511238014: 0.0},\n",
       " 'min_tpr': 0.0,\n",
       " 'max_tpr': 0.0,\n",
       " 'avg_tpr': 0.0,\n",
       " 'asymptotic_surprise': nan,\n",
       " 'significance': nan}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_evaluation_metrics(st_graph, coms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "networkx.classes.digraph.DiGraph"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(st_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{775072732916572160: [775072732916572160],\n",
       " 3974634941: [3974634941],\n",
       " 928458410013675521: [928458410013675521],\n",
       " 848606923087908864: [848606923087908864],\n",
       " 3228848510: [3228848510],\n",
       " 2747410528: [2747410528],\n",
       " 767430013616394240: [767430013616394240],\n",
       " 856578556427632643: [856578556427632643],\n",
       " 2298700003: [2298700003],\n",
       " 134787107: [134787107],\n",
       " 4693956132: [4693956132],\n",
       " 4090167852: [4090167852],\n",
       " 2338760659: [2338760659],\n",
       " 345056881: [345056881],\n",
       " 2251175080: [2251175080],\n",
       " 327467138: [327467138],\n",
       " 2287341380: [2287341380],\n",
       " 2982624985: [2982624985],\n",
       " 2381452592: [2381452592],\n",
       " 2852480889: [2852480889]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[775072732916572160,\n",
       " 3974634941,\n",
       " 928458410013675521,\n",
       " 848606923087908864,\n",
       " 3228848510,\n",
       " 2747410528,\n",
       " 767430013616394240,\n",
       " 856578556427632643,\n",
       " 2298700003,\n",
       " 134787107,\n",
       " 4693956132,\n",
       " 4090167852,\n",
       " 2338760659,\n",
       " 345056881,\n",
       " 2251175080,\n",
       " 327467138,\n",
       " 2287341380,\n",
       " 2982624985,\n",
       " 2381452592,\n",
       " 2852480889]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influential_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_topic_vec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
